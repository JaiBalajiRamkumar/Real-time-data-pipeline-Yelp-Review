{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark==3.5.2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T06:53:35.962813Z","iopub.execute_input":"2025-02-02T06:53:35.963116Z","iopub.status.idle":"2025-02-02T06:54:12.341064Z","shell.execute_reply.started":"2025-02-02T06:53:35.963089Z","shell.execute_reply":"2025-02-02T06:54:12.340232Z"}},"outputs":[{"name":"stdout","text":"Collecting pyspark==3.5.2\n  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.5.2) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812366 sha256=ae5664796f0bb431998641c27ab716436992478d4cb020ab0b4e4e578be3e77a\n  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\nSuccessfully built pyspark\nInstalling collected packages: pyspark\n  Attempting uninstall: pyspark\n    Found existing installation: pyspark 3.5.3\n    Uninstalling pyspark-3.5.3:\n      Successfully uninstalled pyspark-3.5.3\nSuccessfully installed pyspark-3.5.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import from_json, col, udf\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\nfrom transformers import pipeline\nimport logging\nimport os\n\nlogging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n\ncheckpoint_dir = \"/kaggle/working/checkpoints/kafka_to_mongo\"\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n    \nconfig = {\n    \"kafka\": {\n    \"bootstrap.servers\":\"pkc-921jm.us-east-2.aws.confluent.cloud:9092\",\n    \"security.protocol\":\"SASL_SSL\",\n    \"sasl.mechanisms\":\"PLAIN\",\n    \"sasl.username\":\"M2YWR6HS72YUNDIX\",\n    \"sasl.password\":\"AJR15A/m51EqJ+/HDMH7OA7aXuihdL3hx45ZbrAgkGShXq04776YXML6iKtCJ6Ou\",\n    \"client.id\":\"json-serial-producer\"\n},\n    \"mongodb\": {\n        \"uri\":\"mongodb+srv://spark:spark123123@yelp-cluster.1nxmu.mongodb.net/?retryWrites=true&w=majority&appName=yelp-cluster\",\n        \"database\":\"yelpdb\",\n        \"collection\":\"enriched_reviews_collection\"\n    }\n}\n\nsentiment_pipeline = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n\ndef analyze_sentiment(text):\n    if text and isinstance(text, str):\n        try:\n            result = sentiment_pipeline(text)[0]\n            return result['label']\n        except Exception as e:\n            logging.error(f\"Error in sentiment analysis: {e}\")\n            return \"Error\"\n    return \"Empty or Invalid\"\n\nsentiment_udf = udf(analyze_sentiment, StringType())\n\ndef read_from_kafka_and_write_to_mongo(spark):\n    topic = \"raw_data_topic\"\n    \n    schema = StructType([\n        StructField(\"review_id\",StringType()),\n        StructField(\"user_id\",StringType()),\n        StructField(\"business_id\",StringType()),\n        StructField(\"stars\",FloatType()),\n        StructField(\"useful\",IntegerType()),\n        StructField(\"funny\",IntegerType()),\n        StructField(\"cool\",IntegerType()),\n        StructField(\"text\",StringType()),\n        StructField(\"date\",StringType())\n    ])\n    \n    stream_df = (spark.readStream\n                 .format(\"kafka\")\n                 .option(\"kafka.bootstrap.servers\",config['kafka']['bootstrap.servers'])\n                 .option(\"subscribe\",topic)\n                 .option(\"kafka.security.protocol\", config['kafka']['security.protocol'])\n                 .option(\"kafka.sasl.mechanism\",config['kafka']['sasl.mechanisms'])\n                 .option(\"kafka.sasl.jaas.config\",\n                        f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{config[\"kafka\"][\"sasl.username\"]}\" '\n                        f'password=\"{config[\"kafka\"][\"sasl.password\"]}\";')\n                 .option(\"failOnDataLoss\",\"false\")\n                 .load()\n                )\n    parsed_df = stream_df.select(from_json(col('value').cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n    \n    enriched_df = parsed_df.withColumn(\"sentiment\", sentiment_udf(col('text')))\n    \n    query = (enriched_df.writeStream\n             .format(\"mongodb\")\n             .option(\"spark.mongodb.connection.uri\", config['mongodb']['uri'])\n             .option(\"spark.mongodb.database\", config['mongodb']['database'])\n             .option(\"spark.mongodb.collection\", config['mongodb']['collection'])\n             .option(\"checkpointLocation\", checkpoint_dir)\n             .outputMode(\"append\")\n             .start()\n             .awaitTermination()\n            )\n    \nif __name__ == \"__main__\":\n    spark = (SparkSession.builder\n          .appName(\"KafkaStreamToMongo\")\n          .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.2,org.mongodb.spark:mongo-spark-connector_2.12:10.4.0\")\n          .getOrCreate()\n          )\n    read_from_kafka_and_write_to_mongo(spark)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}